---
title: "Sentiment Analysis - Exploration"
---

# VADER Sentiment Analysis
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}  
library(dplyr)
library(vader)
library(parallel)
library(ggplot2)
library(corrplot)
library(stringr)
```

## Introduction
A comparison in rating prediction accuracy based on Lexicon based sentiment analysis.
The following sentiment analysis methodolgies are used: 
* VADER

# Grammar Value Extraction
## Load data
```{r}
# Load data
drugs_review <- read.csv('dataset/drugs_cleaned.csv', stringsAsFactors = FALSE, na.strings = c("NA", ""), encoding = "UTF-8") 
review <- drugs_review$review
df <- drugs_review[, c("rating_type", 'review', "drugName", "condition")]
```

```{r}
# get word count for each review
df$word_count <- review %>% str_count(" ") + 1
# get exclamation count for each review
df$exclamation_count <- review %>% str_count("!")
# get question mark count for each review
df$question_mark_count <- review %>% str_count("\\?")
# get sentence count for each review
df$sentence_count <- review %>% str_count("\\.")
# get average word length for each review
df$avg_word_length <- review %>% str_length() / df$word_count

```

```{r}
# Show df
head(df)
write.csv(df, file = "processed/drug_reviews_ge.csv",FALSE, quote = TRUE, row.names = FALSE )
```

## How do lexicon based sentiment analysis work?
The key to understanding how lexicon based sentiment analysis (SA) works is in the word lexicon.
A lexicon refers to a dictionary of words and their definition, in the context of SA 
it is a dictionary of words and their sentiment. Various SA methods explain the sentiment's polarity 
in different ways, a count of positive and negative words, a number that represents how positive or
negative a word is, or a combination of both. 


```{r}  
# Load the data
drug_review <- read.csv("processed/drug_reviews_ge.csv")
# Only birth control reviews
dim(drug_review)
```

# Vader Sentiment Extraction
Valence Aware Dictionary and sEntiment Reasoner (VADER)
Created by C.J. Hutto and Eric Gilbert at the University of Alabama at Birmingham. VADER 
is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments 
expressed in social media. It is fully open-sourced under the MIT License. 

```{r}
sample_review <- review[5]
sample_score <- drug_review$rating_type[5]
sprintf("% s | % s", sample_score, sample_review)
```

The get_vader calculates the valence of the text passed to it. The valence is a number between -1 and 1.
The closer the number is to 1, the more positive the text is. The closer the number is to -1, the more negative the text is.
The arguments of the function are:
* text: the text to be analyzed
* incl_nt: whether to include rare negation words in the analysis 
* neu_set: whether neutral words count to the calculation 
* rm_qm : whether to remove quotation marks from the text
```{r}
result <- get_vader(sample_review, incl_nt = FALSE, neu_set = TRUE, rm_qm = TRUE)
print(result[-1])
```

## Estimating runtime 
Sentiment analysis is computationally intensive, and can take a long time to run.
The following code estimates the time it takes to run the function on the entire dataset.
The function is run on a sample of the dataset, and the time it takes to run the function is: 6398 seconds. 
```{r, eval=FALSE}
# Estimate the time it takes to run the function
sample_size <- 100
sample <- drug_review[1:sample_size, ]

start <- Sys.time()
sample_vader <- vader_df(sample$review, rm_qm = TRUE)
end <- Sys.time()
result <- (end - start) * (dim(drug_review)[1] / sample_size)

sprintf("Estimated time to run the function: %s", result)
```

6398 seconds is 1hr 46min. Too long for confort. 
In an effort to reduce the time it takes to run the function, parallel computing is used.

```{r, eval=FALSE}
cores <- detectCores()
cl <- makeCluster(cores)

start <- Sys.time()
sample_vader <- parLapply(cl, sample$review, get_vader)
end <- Sys.time()

stopCluster(cl)

result <- (end - start) * (dim(drug_review)[1] / sample_size)
sprintf("Estimated time to run the function: %s", result)
```

## Parallel computing 
With parallel computing we can reduce the time it takes to run 
the function to an estimate of 1,359 seconds; aka 22 minutes.

```{r, eval=FALSE}
cores <- detectCores()
cl <- makeCluster(cores)

vader <- parLapply(cl, drug_review$review, get_vader)

stopCluster(cl)

# Convert the list to a dataframe
vdf <- do.call(rbind, vader)
vdf <- as.data.frame(vdf)
vdf <- vdf[, -1]

# Print first 5 rows
head(vdf)

# Save the dataframe
saveRDS(vdf, "processed/vdf.rds")
```


```{r}
vdf <- readRDS("processed/vdf.rds")

# Cast all columns to numeric
vdf <- vdf %>% dplyr::mutate_if(is.character, as.numeric)
head(vdf)
saveRDS(vdf, "processed/vdf.rds")

# Check for NA values
sum(is.na(vdf))

colnames(drug_review)
df <- drug_review[, c("rating", "drugName", "condition")]
df <- merge(df, vdf, by = "row.names")[, -1]


# Drop NA rows
df <- df[complete.cases(df), ]

head(df)
```

```{r}
ratings <- df %>%
  dplyr::group_by(rating) %>%
  dplyr::summarise(
    compound = mean(compound),
    neg = mean(neg),
    neu = mean(neu),
    pos = mean(pos)
  )

View(ratings)
```

```{r}
# Graph ratings to compound
ggplot(ratings, aes(x = rating, y = compound)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Average Compound Score by Rating",
    x = "Rating",
    y = "Compound"
  )

ggsave("plots/rating_compound.png", width = 5, height = 3, dpi = 300)
```


```{r}
cm <- cor(df[, c("rating", "neg", "neu", "pos", "compound")])

png_corr_matrix <- function(df, file_name, title, height = 650, width = 575) {
  png(height = height, width = width, file = file_name)
  corrplot(df,
    title = " ",
    method = "color",
    type = "upper",
    addCoef.col = "black",
    tl.col = "black",
    diag = FALSE,
    sig.level = 0.01,
    insig = "blank",
    tl.srt = 45,
  )
  mtext(title, at = 3.5, line = 0, cex = 2)
  dev.off()
}

png_corr_matrix(
  df = cm,
  file_name = "images/vdr_corr_matrix.png",
  title = "VADER Correlation Matrix"
)
```


# Performance
```{r}
library(caret)
library(dplyr)
library(caTools)
library(DMwR)
library(e1071)
library(parallel)

# Function to retrieve the data
get_data <- function(drug_reviews, sentiment_rds) {
    # Read the data
    rds <- readRDS(sentiment_rds)
    ge <- read.csv(drug_reviews)
    # Merge the data
    data <- cbind(ge, rds)
    # Return the data
    return(data)
}

# Function to return data with only 'Birth Control' reviews
filter_birth_control <- function(data) {
    data <- data[data$condition == "Birth Control", ]
    return(data)
}

# Function to remove columns
remove_columns <- function(data) {
    data <- select(data, 1, -2:-9, 10:ncol(data))
    return(data)
}

# Function to format the data types
format_dtypes <- function(data) {
    data$rating_type <- as.factor(data$rating_type)
    data <- data %>% mutate_if(is.integer, as.numeric)
    return(data)
}

# Function to filter out bad values
filter_out_bad_vals <- function(data) {
    bad_rows <- apply(
        data, 1,
        function(x) any(is.na(x)) | any(is.nan(x)) | any(is.infinite(x))
    )
    clean_df <- data[!bad_rows, ]
    return(clean_df)
}

# Function to split the data
split_data <- function(data, split_ratio = 0.7, stratified = TRUE) {
    # TODO: Generalise target variable
    if (stratified) {
        # Split with createDataPartition function
        indexes <- createDataPartition(
            data$rating_type,
            p = split_ratio,
            list = FALSE,
            times = 1
        )
        train_data <- data[indexes, ]
        test_data <- data[-indexes, ]
        return(list(train_data, test_data))
    }

    # Split randomly
    if(is.null(data)) {
        stop("The dataframe is empty.")
    }
    tot_rows <- nrow(data)
    indexes <- sample(1:tot_rows, size = 0.8 * nrow(data))
    train_data <- data[indexes, ]
    test_data <- data[-indexes, ]
    return(list(train_data, test_data))
}


# Function to scale and center the data
scale_and_center <- function(train_test) {
    train_data <- train_test[[1]]
    test_data <- train_test[[2]]

    print(head(train))
    preprocess <- preProcess(train_data[, -1], method = c("center", "scale"))
    train[, -1] <- predict(preprocess, train[, -1])
    test[, -1] <- predict(preprocess, test[, -1])
    return(list(train_data, test_data))
}

# Function to perform SMOTE on the data
smote <- function(train_test, perc_over = 100, k = 5) {
    train_data <- train_test[[1]]
    test_data <- train_test[[2]]
    train_data <- SMOTE(
        rating_type ~ .,
        data = train_data,
        perc.over = perc_over,
        k = k
    )
    return(list(train_data, test_data))
}

# Function to perform stratified k-fold cross-validation
stratified_k_fold_cv <- function(train_test,
                                 k = 10,
                                 parallel = FALSE) {
    control <- trainControl(
        method = "cv",
        number = k,
        classProbs = TRUE,
        allowParallel = parallel
    )
    return(control)
}

train_model <- function(control,
                        train_test,
                        method = "svmLinear",
                        preProcess = c(),
                        verbose = FALSE) {
    train_data <- train_test[[1]]
    model <- train(
        rating_type ~ .,
        data = train_data,
        method = method,
        trControl = control,
        preProcess = preProcess
    )
    if (verbose) {
        print(model)
    }
    return(model)
}

split_data <- function(data, split_ratio = 0.7, stratified = TRUE) {
    # TODO: Generalise target variable
    if (stratified) {
        # Split with createDataPartition function
        indexes <- createDataPartition(
            data$rating_type,
            p = split_ratio,
            list = FALSE,
            times = 1
        )
        train_data <- data[indexes, ]
        test_data <- data[-indexes, ]
        return(list(train_data, test_data))
    }

    # Split randomly
    if(is.null(data)) {
        stop("The dataframe is empty.")
    }
    tot_rows <- nrow(data)
    indexes <- sample(1:tot_rows, size = 0.8 * nrow(data))
    train_data <- data[indexes, ]
    test_data <- data[-indexes, ]
    return(list(train_data, test_data))
}

evaluate_model <- function(model, train_test) {
    test_data <- train_test[[2]]
    predictions <- predict(model, test_data)
    confusion_matrix <- confusionMatrix(predictions, test_data$rating_type)
    return (confusion_matrix) 
}
```


```{r}
# Get the data
data <- get_data("processed/drug_reviews_ge.csv", "processed/vdf.rds") %>%
  filter_birth_control() %>%
  remove_columns() %>%
  format_dtypes() %>%
  filter_out_bad_vals()

train_test <- split_data(data, split_ratio = 0.8, stratified = TRUE)
```

```{r}
# Linear SVM 
result <- train_test %>%
  stratified_k_fold_cv(k = 10) %>%
  train_model(train_test, preProcess =  c("center", "scale")) %>%
  evaluate_model(train_test)

result_gbm <- train_test %>%
  stratified_k_fold_cv(k = 10) %>%
  train_model(train_test, method = "gbm") %>%
  evaluate_model(train_test)
  ```