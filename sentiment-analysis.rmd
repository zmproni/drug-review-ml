---
title: "Sentiment Analysis - Exploration"
---

# Sentiment Analysis
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}  
library(dplyr)
library(vader)
library(parallel)
library(ggplot2)
library(corrplot)
library(stringr)
```

## Introduction
A comparison in rating prediction accuracy based on Lexicon based sentiment analysis.
The following sentiment analysis methodolgies are used: 
* VADER

# Grammar Value Extraction
## Load data
```{r}
# Load data
drugs_review <- read.csv('processed/drug_review_cleaned.csv', stringsAsFactors = FALSE, na.strings = c("NA", ""), encoding = "UTF-8") 
review <- drugs_review$review
df <- drugs_review[, c("rating_type", 'review', "drugName", "condition")]
```

```{r}
# get word count for each review
df$word_count <- review %>% str_count(" ") + 1
# get exclamation count for each review
df$exclamation_count <- review %>% str_count("!")
# get question mark count for each review
df$question_mark_count <- review %>% str_count("\\?")
# get sentence count for each review
df$sentence_count <- review %>% str_count("\\.")
# get average word length for each review
df$avg_word_length <- review %>% str_length() / df$word_count

```

```{r}
# Show df
head(df)
write.csv(df, file = "processed/drug_reviews_ge.csv",FALSE, quote = TRUE, row.names = FALSE )
```

## How do lexicon based sentiment analysis work?
The key to understanding how lexicon based sentiment analysis (SA) works is in the word lexicon.
A lexicon refers to a dictionary of words and their definition, in the context of SA 
it is a dictionary of words and their sentiment. Various SA methods explain the sentiment's polarity 
in different ways, a count of positive and negative words, a number that represents how positive or
negative a word is, or a combination of both. 


```{r}  
# Load the data
drug_review <- read.csv("processed/drug_reviews_ge.csv")
# Only birth control reviews
dim(drug_review)
```

# Vader Sentiment Extraction
Valence Aware Dictionary and sEntiment Reasoner (VADER)
Created by C.J. Hutto and Eric Gilbert at the University of Alabama at Birmingham. VADER 
is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments 
expressed in social media. It is fully open-sourced under the MIT License. 

```{r}
sample_review <- review[5]
sample_score <- drug_review$rating_type[5]
sprintf("% s | % s", sample_score, sample_review)
```

The get_vader calculates the valence of the text passed to it. The valence is a number between -1 and 1.
The closer the number is to 1, the more positive the text is. The closer the number is to -1, the more negative the text is.
The arguments of the function are:
* text: the text to be analyzed
* incl_nt: whether to include rare negation words in the analysis 
* neu_set: whether neutral words count to the calculation 
* rm_qm : whether to remove quotation marks from the text
```{r}
result <- get_vader(sample_review, incl_nt = FALSE, neu_set = TRUE, rm_qm = TRUE)
print(result[-1])
```

## Estimating runtime 
Sentiment analysis is computationally intensive, and can take a long time to run.
The following code estimates the time it takes to run the function on the entire dataset.
The function is run on a sample of the dataset, and the time it takes to run the function is: 6398 seconds. 
```{r, eval=FALSE}
# Estimate the time it takes to run the function
sample_size <- 100
sample <- drug_review[1:sample_size, ]

start <- Sys.time()
sample_vader <- vader_df(sample$review, rm_qm = TRUE)
end <- Sys.time()
result <- (end - start) * (dim(drug_review)[1] / sample_size)

sprintf("Estimated time to run the function: %s", result)
```

6398 seconds is 1hr 46min. Too long for confort. 
In an effort to reduce the time it takes to run the function, parallel computing is used.

```{r, eval=FALSE}
cores <- detectCores()
cl <- makeCluster(cores)

start <- Sys.time()
sample_vader <- parLapply(cl, sample$review, get_vader)
end <- Sys.time()

stopCluster(cl)

result <- (end - start) * (dim(drug_review)[1] / sample_size)
sprintf("Estimated time to run the function: %s", result)
```

## Parallel computing 
With parallel computing we can reduce the time it takes to run 
the function to an estimate of 1,359 seconds; aka 22 minutes.

```{r, eval=FALSE}
cores <- detectCores()
cl <- makeCluster(cores)

vader <- parLapply(cl, drug_review$review, get_vader)

stopCluster(cl)

# Convert the list to a dataframe
vdf <- do.call(rbind, vader)
vdf <- as.data.frame(vdf)
vdf <- vdf[, -1]

# Print first 5 rows
head(vdf)

# Save the dataframe
saveRDS(vdf, "processed/vdf.rds")
```


```{r}
vdf <- readRDS("processed/vdf.rds")

# Cast all columns to numeric
vdf <- vdf %>% dplyr::mutate_if(is.character, as.numeric)
head(vdf)
saveRDS(vdf, "processed/vdf.rds")

# Check for NA values
sum(is.na(vdf))

colnames(drug_review)
df <- drug_review[, c("rating", "drugName", "condition")]
df <- merge(df, vdf, by = "row.names")[, -1]


# Drop NA rows
df <- df[complete.cases(df), ]

head(df)
```

```{r}
ratings <- df %>%
  dplyr::group_by(rating) %>%
  dplyr::summarise(
    compound = mean(compound),
    neg = mean(neg),
    neu = mean(neu),
    pos = mean(pos)
  )

View(ratings)
```

```{r}
# Graph ratings to compound
ggplot(ratings, aes(x = rating, y = compound)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = "Average Compound Score by Rating",
    x = "Rating",
    y = "Compound"
  )

ggsave("images/rating_compound.png", width = 5, height = 3, dpi = 300)
```


```{r}
cm <- cor(df[, c("rating", "neg", "neu", "pos", "compound")])

png_corr_matrix <- function(df, file_name, title, height = 650, width = 575) {
  png(height = height, width = width, file = file_name)
  corrplot(df,
    title = " ",
    method = "color",
    type = "upper",
    addCoef.col = "black",
    tl.col = "black",
    diag = FALSE,
    sig.level = 0.01,
    insig = "blank",
    tl.srt = 45,
  )
  mtext(title, at = 3.5, line = 0, cex = 2)
  dev.off()
}

png_corr_matrix(
  df = cm,
  file_name = "images/vdr_corr_matrix.png",
  title = "VADER Correlation Matrix"
)
```
