---
title: "Sentiment Analysis - Exploration"
---

# Sentiment Analysis
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
A comparison in rating prediction accuracy based on Lexicon based sentiment analysis.
The following sentiment analysis methodolgies are used: 
* AFINN
* bing
* nrc
* VADER

## How do lexicon based sentiment analysis work?
The key to understanding how lexicon based sentiment analysis (SA) works is in the word lexicon.
A lexicon refers to a dictionary of words and their definition, in the context of SA 
it is a dictionary of words and their sentiment. Various SA methods explain the sentiment's polarity 
in different ways, a count of positive and negative words, a number that represents how positive or
negative a word is, or a combination of both. 

```{r}  
library(tidyverse)
library(vader)
library(parallel)
```

```{r}  
# Load the data
drug_review <- read.csv("processed/drug_review_cleaned.csv")
dim(drug_review)
```

# Vader Sentiment Extraction
Valence Aware Dictionary and sEntiment Reasoner (VADER)
Created by C.J. Hutto and Eric Gilbert at the University of Alabama at Birmingham. VADER 
is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments 
expressed in social media. It is fully open-sourced under the MIT License. 

```{r}
sample_review <- drug_review$review[5]
sample_score <- drug_review$rating[5]
sprintf("% s | % s", sample_score, sample_review)
```

The get_vader calculates the valence of the text passed to it. The valence is a number between -1 and 1.
The closer the number is to 1, the more positive the text is. The closer the number is to -1, the more negative the text is.
The arguments of the function are:
* text: the text to be analyzed
* incl_nt: whether to include rare negation words in the analysis 
* neu_set: whether neutral words count to the calculation 
* rm_qm : whether to remove quotation marks from the text
```{r}
result <- get_vader(sample_review, incl_nt = FALSE, neu_set = TRUE, rm_qm = TRUE)
print(result[-1])
```

## Estimating runtime 
Sentiment analysis is computationally intensive, and can take a long time to run.
The following code estimates the time it takes to run the function on the entire dataset.
The function is run on a sample of the dataset, and the time it takes to run the function is: 6398 seconds. 
```{r}
# Estimate the time it takes to run the function
sample_size <- 100
sample <- drug_review[1:sample_size, ]

start <- Sys.time()
sample_vader <- vader_df(sample$review, rm_qm = TRUE)
end <- Sys.time()
result <- (end - start) * (dim(drug_review)[1] / sample_size)

sprintf("Estimated time to run the function: %s", result)
```

6398 seconds is 1hr 46min. Too long for confort. 
In an effort to reduce the time it takes to run the function, parallel computing is used.

```{r}
cores <- detectCores()
cl <- makeCluster(cores)

start <- Sys.time()
sample_vader <- parLapply(cl, sample$review, get_vader)
end <- Sys.time()

stopCluster(cl)

result <- (end - start) * (dim(drug_review)[1] / sample_size)
sprintf("Estimated time to run the function: %s", result)
```

## Parallel computing 
With parallel computing we can reduce the time it takes to run 
the function to an estimate of 1,359 seconds; aka 22 minutes.

```{r}
cores <- detectCores()
cl <- makeCluster(cores)

vader <- parLapply(cl, drug_review$review, get_vader)

stopCluster(cl)
```

```{r}
# Convert the list to a dataframe
vader_df <- do.call(rbind, vader)
vader_df <- as.data.frame(vader_df)
vader_df <- vader_df[, -1]

# Print first 5 rows
head(vader_df)

# Save the dataframe
write.csv(vader_df, "processed/vader_df.csv")
```

